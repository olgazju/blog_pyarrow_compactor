{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390cbc68",
   "metadata": {},
   "source": [
    "## Local filesystem, without batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ffee07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 696 input files\n"
     ]
    }
   ],
   "source": [
    "from pyarrow import fs\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "def list_parquet_files(local_fs, path):\n",
    "    file_infos = local_fs.get_file_info(fs.FileSelector(path, recursive=True))\n",
    "    parquet_files = [\n",
    "        info.path\n",
    "        for info in file_infos\n",
    "        if info.type == fs.FileType.File and info.path.endswith(\".parquet\")\n",
    "    ]\n",
    "    return parquet_files\n",
    "\n",
    "local_fs = fs.LocalFileSystem()\n",
    "\n",
    "files = list_parquet_files(local_fs, \"demo_data\")\n",
    "print(f\"Found {len(files)} input files\")\n",
    "\n",
    "dataset = ds.dataset(\n",
    "        files,\n",
    "        format=\"parquet\",\n",
    "        filesystem=local_fs,\n",
    "    )\n",
    "\n",
    "min_rows_per_group = 500\n",
    "max_rows_per_file = 1000000\n",
    "max_rows_per_group = 10000\n",
    "\n",
    "ds.write_dataset(\n",
    "    dataset,\n",
    "    \"output_demo_data\",\n",
    "    format=\"parquet\",\n",
    "    basename_template=f\"compacted_{{i}}.parquet\",\n",
    "    min_rows_per_group=min_rows_per_group,\n",
    "    max_rows_per_file=max_rows_per_file,\n",
    "    max_rows_per_group=max_rows_per_group,\n",
    "    existing_data_behavior=\"overwrite_or_ignore\",\n",
    "    use_threads=True,\n",
    "    filesystem=fs.LocalFileSystem(),\n",
    "    file_options=ds.ParquetFileFormat().make_write_options(compression=\"gzip\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a45e7ac",
   "metadata": {},
   "source": [
    "## Local filesystem, with batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661916d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 696 input files\n",
      "Processing files 0 to 100\n",
      "Processing files 100 to 200\n",
      "Processing files 200 to 300\n",
      "Processing files 300 to 400\n",
      "Processing files 400 to 500\n",
      "Processing files 500 to 600\n",
      "Processing files 600 to 696\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.dataset as ds\n",
    "from pyarrow import fs\n",
    "import os\n",
    "\n",
    "def list_parquet_files(local_fs, path):\n",
    "    file_infos = local_fs.get_file_info(fs.FileSelector(path, recursive=True))\n",
    "    return [\n",
    "        info.path\n",
    "        for info in file_infos\n",
    "        if info.type == fs.FileType.File and info.path.endswith(\".parquet\")\n",
    "    ]\n",
    "\n",
    "\n",
    "input_path = 'demo_data'\n",
    "output_path = 'output_demo_data_batched'\n",
    "files_per_batch = 100\n",
    "min_rows_per_group = 500\n",
    "max_rows_per_file = 1000000\n",
    "max_rows_per_group = 10000\n",
    "\n",
    "local_fs = fs.LocalFileSystem()\n",
    "\n",
    "all_files = list_parquet_files(local_fs, input_path)\n",
    "print(f\"Found {len(all_files)} input files\")\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "for batch_idx in range(0, len(all_files), files_per_batch):\n",
    "    batch_files = all_files[batch_idx: batch_idx + files_per_batch]\n",
    "    print(f\"Processing files {batch_idx} to {batch_idx + len(batch_files)}\")\n",
    "\n",
    "    dataset = ds.dataset(batch_files, format=\"parquet\", filesystem=local_fs)\n",
    "\n",
    "    ds.write_dataset(\n",
    "        dataset,\n",
    "        output_path,\n",
    "        format=\"parquet\",\n",
    "        basename_template=f\"compacted_batch_{batch_idx // files_per_batch}_{{i}}.parquet\",\n",
    "        min_rows_per_group=min_rows_per_group,\n",
    "        max_rows_per_file=max_rows_per_file,\n",
    "        max_rows_per_group=max_rows_per_group,\n",
    "        existing_data_behavior=\"overwrite_or_ignore\",\n",
    "        use_threads=True,\n",
    "        filesystem=local_fs,\n",
    "        file_options=ds.ParquetFileFormat().make_write_options(compression=\"gzip\"),\n",
    "    )\n",
    "\n",
    "    del dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog_pyarrow_compactor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
